[{"title":"Scrapy框架(二)","url":"/2018/10/28/Scrapy框架(二)/","content":"\n从实例中看scrapy框架中各部分的功能\n\n![](https://wx1.sinaimg.cn/mw690/6c3e6b13gy1fwjoq4i6rcj20zk0wk4qp.jpg)\n\n<!--more-->\n\n取的目标网站为：https://image.so.com\n\n打开浏览器开发者工具，过滤器切换到XHR选项,下拉页面，可以看到下面就会呈现许多Ajax请求\n\n![](https://ask.qcloudimg.com/http-save/developer-news/q8remsppeb.jpeg)\n\n![](https://ask.qcloudimg.com/http-save/developer-news/7xvqjdt74l.jpeg)\n\n返回格式是JSON。其中字段就是一张张图片的详情信息，包含了30张图片的ID、名称、链接、缩略图等信息。另外观察Ajax请求的参数信息，有一个参数一直在变化，这个参数很明显就是偏移量。当为30时，返回的是前30张图片，sn为60时，返回的就是第31~60张图片。\n\n实际上，我们向浏览器发送ajax请求，我们从网址上，即被url编码的params列表，比如：\n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwo4qvh794j311b0bu75u.jpg)\n\n我们看到了:\n\n1. ch\n2. sn\n3. listtype\n4. temp\n\n所以所谓的参数列表\n\n```\nparams{\n    xxx\n    xxx\n    xxx\n}\n```\n\n只用含有这几个参数就可以了。\n\n____\n\n分析完毕后，开始新建项目.\n\n```\nscrapy startproject images360\n```\n\ncd 到这个项目的 spiders项目下，创建一个爬虫名为`images360`\n\n```\nscrapy genspider images360  images.so.com\n```\n\n**有了爬虫，接下来应该做什么？**🧐\n\n## 第一 ##\n\n肯定是要对spiders处理，我们进入spiders文件夹里面的`images.py`里看，爬虫有了，方法要我们创建。\n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwo522pfeyj30ld0ic0ti.jpg)\n\n这里的爬虫有三个部分:\n\n*1* :  `name`是爬虫名，`allowed_domain `是访问的域名，`start_urls`的含义是过滤爬取的域名，在插件OffsiteMiddleware启用的情况下（默认是启用的），不在此允许范围内的域名就会被过滤，而不会进行爬取,\n\n请注意,以上三个部分，都是在我们第二个命令执行时，S框架自动生成的，我们不用动.\n\n*2*: `start_requests`就算是我们熟悉的通过循环向浏览器发送ajax请求，`parse`是对返回的response进行解析，我们知道，基本的爬虫实现都是在这里做的。\n\n\n\n我们重点关注其他部分，也就是框架的其他环节\n\n```\nitem\nmiddleware\nsettings\npiplines\n```\n\n`item`:打个比方，你想要你爬到的东西的什么部分？你希望他们以什么样子交给你？\n\n​               我想要网站图片的id,链接地址，主题，缩略图,我想要它们以一个字典的形式呈交给我\n\n​              那么，item.py就是干这个的.​           \n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwo5bmi8y8j30h80ai3ym.jpg)\n\n`middleware`:中间件，作用是修改代理ip啦，访问浏览器的user_agent啦，[了解一下](https://blog.csdn.net/yancey_blog/article/details/53896092) ,此例不用middleware     \n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwo6sjw5saj30mq0p80ua.jpg)","tags":["爬虫"]},{"title":"Ajax和XHR","url":"/2018/10/28/Ajax和XHR/","content":"\n  ajax是asynchronous javascript and XML的简写\n\n<!--more--->\n\n[拜读](https://www.cnblogs.com/xiaohuochai/p/6036475.html)\n\n<br><br><br><br>","tags":["前端基础"]},{"title":"Scrapy框架解析","url":"/2018/10/26/Scrapy/","content":"\nScrapy是一个非常优秀的爬虫框架\n\n<!--more-->\n\n###  什么是Scrapy？ \n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwml6x6211j30jg0dqdi2.jpg)\n\n`Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中`\n\n### 1 . [先安装](https://www.cnblogs.com/lfoder/p/6565088.html)\n### 2. 第一个项\n`scrapy startproject tutorial` \n直接cmd执行，然后在   c/user/xxx   路径下，出现了一个tutorial 文件夹\n\n\n\n\n\n### Downloader Middleware 🔍\n\nMiddleware 是整个scrapy框架中负责网页下载环节的工作，比如说 我们有一个新的项目，并有个该项目下的spider\n\n```python\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass HttpbinSpider(scrapy.Spider):\n    name = 'httpbin'\n    allowed_domains = ['httpbin.org']\n    start_urls = ['http://httpbin.org/get']\n\n    def parse(self, response):\n        self.logger.debug(response.text) #打印响应\n        self.logger.debug('Status Code: ' + str(response.status))\n```\n\n执行spider\n\n```c&#39;m&#39;d\nscrapy crawl httpbin\n```\n\n发现获取到的内容中含有:\n\n```html\n2018-10-27 08:57:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 439,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 768,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 10, 27, 0, 57, 9, 949044),\n 'log_count/DEBUG': 5,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2018, 10, 27, 0, 57, 8, 773548)}\n```\n\n其中的`headers`中的`User_Agent` 是`DownloaderMiddleware`并利用 process_request（）方法设置的 User_Agent\n\n我们改动一下 `midddlerware.py`，添加一个RandomUserAgentMiddleware类 ，如下\n\n```python\nimport random\n\nclass RandomUserAgentMiddleware(self):\n    def __init__():\n        self.user_agent=[\n        1.代理\n        2.代理\n        3.代理\n        ]\n    def process_request(self,request,spider):\n        request.headers['User_Agent'] = random.choice(self.user_agent)\n```\n\n并且必须在settings.py中取消注释:  `DOWNLOADER_MIDDLEWARES `并设置成如下内容:\n\n```\nDOWNLOADER_USERAGENT{\n    'scrapydownloadermiddtest.middlewares.RandomUserAgentMiddleware': 543,\n}\n```\n\n接下来我们重新运行 Spider，就可以看到 User-Agent 被成功修改为列表中所定义的随机的一个 User-Agent 了\n\n\n\n\n\n Downloader Middleware 还有` process_response()`方法。 Downloader对 Request 执行下载之 后会得到 Response，随后 Scrapy 引擎会将 Response 发送回 Spider进行处理。 但是在 Response 被发送 给 Spider 之前，我们同样可以使用` process_response()`方法对 Response 进行处理\n\n```\ndef process_response(self, request, response, spider):\n    response.status = 201 \n    return response` \n```\n\n我们将 response 变量的 status 属性修改为 201 ，随后将 response 返回，这个被修改后的 Response 就会被发送到 Spider。\n我们再在 Spider里面输出修改后的状态码，在 parse() 方法中添加如下的输出语句：\n\n```\nself.logger.debug（'StatusCode:' + str(response.status)) \n```\n\n重新运行之后，控制台输出了如下内容：\n`[httpbin] DEBUG: Status Code: 201 `\n\n Response 的状态码成功修改了。\n因此要想对 Response 进行后处理，就可以借助于` process_response()`方法\n\n\n\n###  SpiderMiddleware 用法🏳‍🌈\n\nSpider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 Spider中间件(Middleware) \n一句话总结就是：`处理解析部`\n\n### Item PipeLine 用法🐱‍👓\n\n当spider爬取到item后，它被发送到项目管道（Item Pipeline），通过几个组件按顺序进行处理。每一个Item Pipeline是一个实现了简单方法的Python类，它接收到一个item并对其执行一个操作，也要决定该item是否应该继续通过管道，或者被丢弃，不再进行处理。\n\n`简单来说，我们通过爬虫爬取的项目，先交给项目管道，项目管道把数据处理处理(删除一些啦，存到mongodb啦)产生新的item`\n\n**Item Pipeline典型的用途是：**\n1.清理HTML数据\n2.验证爬取的数据(检查items是否包含某些字段)\n3.检查副本(并删除它们)\n4.将item数据存储在数据库中\n\n`每个Item Pipeline都是一个Python类`，它必须实现以下方法:\n\n**process_item(self, item, spider)**\n\n这个方法可以被每个Item Pipeline调用，process_item()必须是:返回一个字典类型数据、返回一个条目(或任何子类)对象，返回一个 Twisted Deferred 或者DropItem异常，丢弃的item不再由进一步的Item Pipeline处理。\n参数含义：\nitem： Item对象或字典，爬取的item\nspider：spider对象，爬取了这个item的spider\n此外，他们还可以实现以下方法:\n\n**open_spider(self, spider)** ->当spider打开时，函数就会被调用，spider参数含义：被打开的spider\n**close_spider(self, spider)**  ->当spider关闭是，函数会被调用\n**from_crawler(cls, crawler)**  -> 如果存在，这个类方法被调用来从一个Crawler创建一个spider实例。它必须返回管道的一个新实例，Crawler对象提供对所有的scrapy核心组件的访问，比如设置和信号;这是管道访问它们并将其功能连接到scrapy的一种方式。\n\n\n\n\n\n\n\n\n\n\n\n----\n\n参考链接:\n\n​              [Scrapy进阶,middleware的使用](https://blog.csdn.net/xnby/article/details/52297047)\n\n​              [运维学Python之爬虫高级篇](http://blog.51cto.com/linuxliu/2068601?wx)\n\n​              ","tags":["爬虫"]},{"title":"不该死的人，还活着","url":"/2018/10/21/nuisance/","content":"我再次体会到将要放弃的感觉，此刻我想的并不是，黑色成为黑色的原因，而是眼前的红色为什么这么刺眼。\n<!--more-->\n\n\n\n\n","tags":["话"]},{"title":"qppium模拟---微信朋友圈","url":"/2018/10/20/wechatspider/","content":"\n初尝Android爬虫\n<!--more-->\n\n## 1. appium介绍 ## \n\n____\n\n[官方文档](https://testerhome.com/wiki/appiumdoccn)\n\n```c\nAppium is an open source test automation framework for use with native and hybrid mobile apps.\n```\n\n如果说我们可以使用Selenium在web端对行为进行模拟，那么Appnium即是我们针对App的操作模拟器。\n\n## 2 . 爬取思路\n___\n\n点击 Start Server按钮即可启动 Appium 的服务，相当于开启了一个 Appium 服务器。 我们可以通过 Appium 内置的驱动或 Python代码向 Appium 的服务器发送一系列操作指令， Appium 就会根据 不同的指令对移动设备进行驱动，完成不同的动作\n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwhfvq2ynqj30m50kiwfv.jpg)\n\nAppium 运行之后正在监听 4723 端口 。 我们可以向此端口对应的服务接口发送操作指令，此页面就会显示这个过程的操作日志\n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwhfw7fi3fj30m30klgls.jpg)\n\n*开始连接*\n\n\n1.Android 手机通过数据线和运行 Appium 的 PC 相连，同时打开 USB 调试功能\n2.cmd 输入`adb devices`，得到设备名称\n3.配置Capability参数(如下图)，其中：\n​     //  platformName： 它是平台名称，需要区分 Android 或 iOS，此处填写 Android  \n​     //  deviceName： 它是设备名称，此处是手机的具体类型。 \n​     //  appPackage： 它是 App 程序包名。通过cmd命令`adb shell pm list packages`可查\n​     //  appActivity： 它是入口 Activity 名，这里通常需要以．开头。\n4.start Session\n\n​                                                    \n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fwhfwunwn5j31160jignb.jpg)\n\n 到这里，我们可以在appium看到每块元素的id，xpath定位等信息， 同时，  tap(),click(),等动作函数，也给我们随心所欲的模拟提供空间。\n\n就是`定位`+`获取`，并把获取内容存入MongoDB\n\n//定位可通过 ID，或直接 XPATH 定位(事实上，appium很人性化的直接为用户提供了某元素的XPATH查找路径)\n\n//截至10.23 0:17 仍未找到获取文本安卓元素对应的文本的方法。\n\n```python\nimport os \nfrom appium import webdriver\nfrom appium.webdriver.common.touch_action import TouchAction\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom pymongo import MongoClient\nfrom time import sleep\nfrom processor import Processor\nfrom config import *\n\n\nclass Moments():\n    def __init__(self):\n        \"\"\"\n        初始化\n        \"\"\"\n        # 驱动配置\n        self.desired_caps = {\n            'platformName': PLATFORM,\n            'deviceName': DEVICE_NAME,\n            'appPackage': APP_PACKAGE,\n            'appActivity': APP_ACTIVITY\n        }\n        self.driver = webdriver.Remote(DRIVER_SERVER, self.desired_caps)\n        self.wait = WebDriverWait(self.driver, TIMEOUT)\n        self.client = MongoClient(MONGO_URL)\n        self.db = self.client[MONGO_DB]\n        self.collection = self.db[MONGO_COLLECTION]\n        # 处理器\n        self.processor = Processor()\n    \n    def login(self):\n        \"\"\"\n        登录微信\n        :return:\n        \"\"\"\n        # 登录按钮\n        login = self.wait.until(EC.presence_of_element_located((By.ID, 'com.tencent.mm:id/d75')))\n        login.click()\n        # 手机输入\n        phone = self.wait.until(EC.presence_of_element_located((By.ID, 'com.tencent.mm:id/hz')))\n        phone.set_text(USERNAME)\n        # 下一步\n        next = self.wait.until(EC.element_to_be_clickable((By.ID, 'com.tencent.mm:id/alr')))\n        next.click()\n        # 密码\n        password = self.wait.until(\n            EC.presence_of_element_located((By.XPATH, '//*[@resource-id=\"com.tencent.mm:id/hz\"][1]')))\n        password.set_text(PASSWORD)\n        # 提交\n        submit = self.wait.until(EC.element_to_be_clickable((By.ID, 'com.tencent.mm:id/alr')))\n        submit.click()\n        submit = self.wait.until(EC.element_to_be_clickable((By.ID, 'com.tencent.mm:id/an3')))\n        submit.click()\n    \n    def enter(self):\n        \"\"\"\n        进入朋友圈\n        :return:\n        \"\"\"\n        # 选项卡\n        tab = self.wait.until(\n            EC.presence_of_element_located((By.XPATH, '//android.widget.FrameLayout[@content-desc=\"当前所在页面,与wxid_br71rgy4kg5n22的聊天\"]/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.ViewGroup/android.widget.FrameLayout[1]/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.RelativeLayout/android.widget.LinearLayout/android.widget.RelativeLayout[3]/android.widget.LinearLayout/android.widget.RelativeLayout/android.widget.ImageView')))\n        tab.click()\n        # 朋友圈\n        moments = self.wait.until(EC.presence_of_element_located((By.ID, 'com.tencent.mm:id/a7f')))\n        moments.click()\n    \n    def crawl(self):\n        \"\"\"\n        爬取\n        :return:\n        \"\"\"\n        while True:\n            # 当前页面显示的所有状态\n            items = self.wait.until(\n                EC.presence_of_all_elements_located(\n                    (By.XPATH,'//android.widget.FrameLayout[@content-desc=\"当前所在页面,朋友圈\"]/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.ViewGroup/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.RelativeLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.ListView/android.widget.FrameLayout')))\n            # 上滑\n            self.driver.swipe(FLICK_START_X, FLICK_START_Y + FLICK_DISTANCE, FLICK_START_X, FLICK_START_Y)\n            # 遍历每条状态\n            for item in items:              \n                try:\n                    # 昵称\n                    nickname = self.wait.until(\n                EC.presence_of_all_elements_located(\n                    (By.XPATH,'//android.widget.FrameLayout[@content-desc=\"当前所在页面,朋友圈\"]/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.ViewGroup/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.RelativeLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.ListView/android.widget.FrameLayout[1]/android.widget.LinearLayout/android.widget.RelativeLayout/android.widget.TextView'))).text()\n                    # 正文\n                    content = self.wait.until(\n                EC.presence_of_all_elements_located(\n                    (By.XPATH,'//android.widget.FrameLayout[@content-desc=\"当前所在页面,朋友圈\"]/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.ViewGroup/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.RelativeLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.ListView/android.widget.FrameLayout[1]/android.widget.LinearLayout/android.widget.LinearLayout[1]/android.widget.LinearLayout/android.view.View'))).text()\n                    # 日期\n                    date = self.wait.until(\n                EC.presence_of_all_elements_located(\n                    (By.XPATH,'//android.widget.FrameLayout[@content-desc=\"当前所在页面,朋友圈\"]/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.view.ViewGroup/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.RelativeLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.ListView/android.widget.FrameLayout[1]/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.LinearLayout/android.widget.TextView'))).text()\n                    # 处理日期\n                    date = self.processor.date(date)\n                    print('一次insert记录')\n                    print(nickname, content, date)\n                    data = {\n                        'nickname': nickname,\n                        'content': content,\n                        'date': date,\n                    }\n                    # 插入MongoDB\n                    self.collection.update({'nickname': nickname, 'content': content}, {'$set': data}, True)\n                    sleep(SCROLL_SLEEP_TIME)\n                except NoSuchElementException:\n                    pass\n    \n    def main(self):\n        \"\"\"\n        入口\n        :return:\n        \"\"\"\n        # 登录\n        self.login()\n        # 进入朋友圈\n        self.enter()\n        # 爬取\n        self.crawl()\n\n\nif __name__ == '__main__':\n    moments = Moments()\n    moments.main()\n\n```\n\n\n\n对朋友圈时间进行处理\n\n```python\nimport time\nimport re\n\n\nclass Processor():\n    def date(self, datetime):\n        \"\"\"\n        处理时间\n        :param datetime: 原始时间\n        :return: 处理后时间\n        \"\"\"\n        if re.match('\\d+分钟前', datetime):\n            minute = re.match('(\\d+)', datetime).group(1)\n            datetime = time.strftime('%Y-%m-%d', time.localtime(time.time() - float(minute) * 60))\n        if re.match('\\d+小时前', datetime):\n            hour = re.match('(\\d+)', datetime).group(1)\n            datetime = time.strftime('%Y-%m-%d', time.localtime(time.time() - float(hour) * 60 * 60))\n        if re.match('昨天', datetime):\n            datetime = time.strftime('%Y-%m-%d', time.localtime(time.time() - 24 * 60 * 60))\n        if re.match('\\d+天前', datetime):\n            day = re.match('(\\d+)', datetime).group(1)\n            datetime = time.strftime('%Y-%m-%d', time.localtime(time.time()) - float(day) * 24 * 60 * 60)\n        return datetime\n```\n\n\n\n\n\n---未完待续\n\n\n\n\n\n\n\n\n\n","tags":["爬"]},{"title":"MongoDB","url":"/2018/10/20/MongoDB/","content":"\n*什么是MongoDB?我们又该去怎样使用它？*\n\n<!--more-->\n\n`已删`\n\n[MongoDB官方文档](https://www.runoob.com/mongodb)\n\n[PyMongo是在Python环境下使用MongoDB的语言](https://blog.csdn.net/callinglove/article/details/45668673?utm_source=blogxgwz0)\n\n写了一半，忽然觉得，针对 那些[工具性的]，就老老实实把官方的文档看懂。\n\n\n\n\n\n\n\n","tags":["工具"]},{"title":"写在学习代理池和cookie池之后","url":"/2018/10/08/sougoupachong/","content":"\n有关基本思路的描述\n\n <!--more-->\n\n这篇博客的目的是，尝试搞懂我最近做了什么.\n\n我们为什么要用代理池？<br>\n\n```\n部分网站有反爬虫措施，当我们使用一个ip地址对该网站进行过多的访问时，会被反爬虫机制自动视为爬虫，从而ip拉黑.\n```\n\n代理池的构建是怎样的？<br>\n\n```\n对网上的免费ip代理网站进行爬取(西刺，proxy360)，它们网站的ip源是完全暴露在网页源码上的，我们只用简单的进行分析，就可以拿到足够的ip.\n```\n\n如此一来，思路很清晰，设计一个爬虫，针对几个免费的网站解析爬取，将拿到 ip 进行筛选，排名，存入数据库。  并可以实现获取接口，即调即用。\n\n`基本模块分为 4块：存储模块、获取模块、检测模块、接口模块`\n\n###\n\n1. 存储模块： 负责存储抓取下来的代理。 首先要保证代理不重复， 要标识代理的可用情况，还 要动态实时处理每个代理，所以一种比较高效和方便的存储方式就是使用 Redis 的 Sorted Set，即有序集合。\n\n2. 获取模块： 需要定时在各大代理网站抓取代理。 代理可以是免费公开代理也可以是付费代 理，代理的形式都是 IP 加端口，此模块尽量从不同来源获取，尽量抓取高匿代理，抓取成功 之后将可用代理保存到数据库中。 \n\n3. 检测模块： 需要定时检测数据库中的代理。 这里需要设置一个检测链接，最好是爬取哪个网 站就检测哪个网站，这样更加有针对性，如果要做一个通用型的代理，那可以设置百度等链 接来检测。 另外，我们需要标识每一个代理的状态，如设置分数标识， 100 分代表可用，分 数越少代表越不可用。 检测一次，如果代理可用，我们可以将分数标识立即设置为 100 满 分，也可以在原基础上加 l分；如果代理不可用，可以将分数标识减 l 分，当分数戚到一定阔 值后，代理就直接从数据库移除。 通过这样的标识分数，我们就可以辨别代理的可用情况， 选用的时候会更有针对性。 \n\n4. 接口模块： 需要用 API 来提供对外服务的接口 。 其实我们可以直接连接数据库采取对应的数 据，但是这样就需要知道数据库的连接信息，并且要配置连接，而比较安全和方便的方式就 是提供一个 Web API 接口，我们通过访问接口即可拿到可用代理。 另外，由于可用代理可能 有多个，那么我们可以设置一个随机返回某个可用代理的接口，这样就能保证每个可用代理 都可以取到，实现负载均衡\n\n![](http://ww1.sinaimg.cn/large/006YPp6gly1fw7uv32bfaj30px0cfahk.jpg)\n\n[我们需要自己造轮子吗](https://github.com/Python3WebSpider/ProxyPool)  我觉得这要分情况。 \n\n<br><br>\n\n我们为什么要使用cookie池\n\n```\ncookies是我们登陆一个页面的接口，不登陆固然可以获取网页源码，但是会受访问限制，以及不具有一些页面的权限，比如微博来说，我们的cookie池实际上就是多个微博账号的信息。\n```\n\n与上面ip池的搭建差别是一些微博账号是我们需要自己通过某些渠道购买的。\n\n[轮子](https://github.com/Python3WebSpider/CookiesPool)\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["爬虫"]},{"title":"python の __new__()","url":"/2018/10/07/python_new/","content":"\n`在学习代理池中，发现了自己搁置的问题。我尝试通过这篇文章，来理清   __ new __  () 和 __ init __()的关系.`\n\n <!--more-->\n\n\n\n```python\n# coding:utf-8\n\n\nclass Foo(object):\n    price = 50\n\n    def how_much_of_book(self, n):\n        print(self)\n        return self.price * n\n\nfoo = Foo()\nprint(foo.how_much_of_book(8))\nprint(dir(Foo))\n```\n\n⬆⬆⬆分析上面的代码，这个类实例化过程，Foo类继承object类，继承了object的__new__方法。当你没有重载这个方法(通俗来说，`你没有在Foo类中没有写__new__方法)`，Foo实例化是默认自动调用父类__new__方法，这个方法返回值为类的实例，也就是self, 用来提供这个函数的第一个参数。\n\n```python\n# coding:utf-8\n\n\nclass Foo(object):\n    price = 50\n\n    def __new__(cls, *agrs, **kwds):\n        inst = object.__new__(cls, *agrs, **kwds)\n        print(inst)\n        return inst\n\n\n    def how_much_of_book(self, n):\n        print(self)\n        return self.price * n\n\nfoo = Foo()\nprint(foo.how_much_of_book(8))\n# <__main__.Foo object at 0x1006f2750>\n# <__main__.Foo object at 0x1006f2750>\n# 400\n```\n\n⬆⬆⬆请看上面代码，Foo类中重载了__new__方法，它的返回值为Foo类的实例对象\n\n___\n\n\n\n`_init__ 方法为初始化方法，为类的实例提供一些属性或完成一些动作`\n\n```python\n# coding:utf-8\n\n\nclass Foo(object):\n\n    def __new__(cls, *agrs, **kwds):\n        inst = object.__new__(cls, *agrs, **kwds)\n        print(inst)\n        return inst\n\n\n    def __init__(self, price=50):\n        self.price = price\n\n    def how_much_of_book(self, n):\n        print(self)\n        return self.price * n\n\nfoo = Foo()\nprint(foo.how_much_of_book(8))\n\n# <__main__.Foo object at 0x1006f2750>\n# <__main__.Foo object at 0x1006f2750>\n# 400\n```\n\n\n\n那么说到这里，我们用两句话来总结就是:\n\n⭐__new__ 方法创建实例对象供__init__ 方法使用，__init__方法定制实例对象。\n\n__⭐new__ 方法必须返回值，__init__方法不需要返回值。(如果返回非None值就报错)\n\n</br></br>\n\n我们举两个例子，观察New的常见两种用法\n\n第一： 继承不可变数据类型时需要用到__new__方法(like int, str, or tuple） \n\n 将英寸转化成米\n\n```\n# coding:utf-8\n\n\nclass Inch(float):\n    \"Convert from inch to meter\"\n    def __new__(cls, arg=0.0):\n        return float.__new__(cls, arg*0.0254)\n\nprint(Inch(12))\n```\n\n</br>\n\n第二:   用在元类，定制创建类对象\n\n```python\n# coding:utf-8\n'''来自http://eli.thegreenplace.net/2011/08/14/python-metaclasses-by-example'''\n\nclass MetaClass(type):\n\n    def __new__(meta, name, bases, dct):\n        print '-----------------------------------'\n        print \"Allocating memory for class\", name\n        print meta\n        print bases\n        print dct\n        return super(MetaClass, meta).__new__(meta, name, bases, dct)\n\n    def __init__(cls, name, bases, dct):\n        print '-----------------------------------'\n        print \"Initializing class\", name\n        print cls\n        print bases\n        print dct\n        super(MetaClass, cls).__init__(name, bases, dct)\n\n\nclass Myclass(object):\n    __metaclass__ = MetaClass\n\n    def foo(self, param):\n        print param\n\n\np = Myclass()\np.foo(\"hello\")\n\n# -----------------------------------\n# Allocating memory for class Myclass\n# <class '__main__.MetaClass'>\n# (<type 'object'>,)\n# {'__module__': '__main__', 'foo': <function foo at 0x1007f6500>, '__metaclass__': <class '__main__.MetaClass'>}\n# -----------------------------------\n# Initializing class Myclass\n# <class '__main__.Myclass'>\n# (<type 'object'>,)\n# {'__module__': '__main__', 'foo': <function foo at 0x1007f6500>, '__metaclass__': <class '__main__.MetaClass'>}\n# hello\n```\n\n`Myclass相当于 拷贝了 MetaClass 的方法，MetaClass 的种类仍然是 MetaClass，定制的init 的种类 也自然是Myclass`","tags":["工具"]},{"title":"关于海","url":"/2018/10/07/photo/","content":"![](https://wx4.sinaimg.cn/mw690/6c3e6b13gy1fvyja7cikaj20uk0kedj0.jpg)\n<!--more-->\n![](https://wx3.sinaimg.cn/mw690/006lRDaTgy1fw2apaw5nlj32ro1ugnpe.jpg)\n</br>\n</br>\n<!--more-->","tags":["话"]},{"title":"GEETEST滑动验证码破解","url":"/2018/10/04/huadongpojie/","content":"验证码的破解，是网站爬虫正常运行所必须具备的功能。\n\n<!--more-->   \n 注: 本文针对[*滑动验证被切割随机打乱，并根据源码中 **< DIV >** 标签顺序进行 **取块排列**  验证情况 ，属于常见的一种混淆方式*]\n\n___\n\n关键词：\n\n1. **<a href=\"#1\" target=\"_self\">图像分析</a>**\n2. **<a href=\"#2\" target=\"_self\">图像拼接</a>**\n\n\n<p id=\"1\"><br>图像分析:</p>\n\n\n   &nbsp;&nbsp; 我们以[虎嗅网](https://www.huxiu.com/)登陆页面为例，我们在输入手机注册前，需要通过一个滑动式的 *captcha* ,我们可以通过`webdriver.chrome  + selenium` 来模拟拖动，问题就在于究竟要拖动多远？拖动的距离究竟是多少？(offset 究竟是多少？)\n\n\n![这是截图](http://ww1.sinaimg.cn/large/006YPp6gly1fvxl234pokj30dg07w76i.jpg)\n\n   &nbsp;&nbsp;仔细一想，如果有一个不存在缺口的**源图片**，那就舒服了，因为那样的话，我们只需比较，`源图哪些像素块，比上面的图形颜色更深，从而判断出深颜色缺口的位置`,这样我们就得到了，需要模拟拖动的距离了。那么从哪里可以得到原始图片呢？\n   F12审查代码时，发现了这样两个< div>:\n\n![黄线标出](http://ww1.sinaimg.cn/large/006YPp6gly1fvxku4epdej31620ew0uu.jpg)\n\n我们打开后发现，里面图片链接全部指向各自的那一个图片，[一个长这样](http://ww1.sinaimg.cn/large/006YPp6gly1fvxkuhcc8ij31630iq41p.jpg),[一个长这样](http://ww1.sinaimg.cn/large/006YPp6gly1fvxkxx3xzuj30kc0doabb.jpg),这，这！这不就是有缺口和没缺口的图片吗？\n<p id=\"2\"> <br><br> 图像拼接:</p>\n等等，为什么乱了？\n\n对，如果我们仔细观察，我们发现图片是被切割打乱了的，而且，我们判断，如果想得到原图，一定是要像拼积木一样把这些小块重新排列组合,那么，有什么规律在其中呢？我们该怎样一个一个拼呢？\n其实，不难发现，我们在打开那两个标签后，我们发现一些整整齐齐的从上到下的标签，并且还有   `position`属性，x,值以12为差，最小1 最大 277，但y只有 0 或 58，由此分析出，从上到下，是我们取积木的顺序，取打乱图的第一个(x,y)坐标点对应处的一块积木，然后拼接到一个空白框的左上角，然后接着从乱图里面取出第二个位置的积木，紧挨着刚才那一个向右，此行满了换下一行，以此类推,最终得到一个完整图。\n这样的话，我们的分析就结束了，然后我们通过像素比较就可以找出，缺口的位置，然后模拟拖动就ok了。\n注意，实际应用中，我们需要实验性模拟拖动几次，再对拖动距离distance进行微修正，从而提高准确率。\n<br><br>\n[源码](https://paste.ubuntu.com/p/y7PSVFdxsJ/)\n<br>\n[参考博客](https://www.jianshu.com/p/c8df1194b514)","tags":["爬虫"]},{"title":"快速熟悉 markdown","url":"/2018/10/02/learn_MarkDown/","content":"*快速熟悉*        \n   ***markdown***\n\n<!--more-->        \n___\n>我们可以使用  **大于号** ，来表示引用\n\n~~我们在文本两边，加上一对双波浪号，就得到删除特效~~\n\n[方框内部是文本,点击跳转百度](http://www.baidu.com)\n![如果框起前面加个感叹号，代表我要加图片图片](http://ww1.sinaimg.cn/large/006YPp6gly1fvu4hphapdj307g08edh6.jpg)\n倘若我先写三个井号，代表我要开始**列表**，*、+号带领无序，1.带领有序\n###\n* k1\n* k2\n* k3\n1. l1\n2. l2\n3. l3\n###\n一对反引号可以生成一个`背景加深`，这很讨喜\n且表格的创建，格外方便，只用添加一点  |:----: |这表明，我正在写表格\n|电影|导演|评分|\n|:----:|:----:|:----:|\n|天注定|贾樟柯|未知|\n|江湖儿女|贾樟柯|未知|\n|三峡好人|贾樟柯|未知|\n|站台|贾樟柯|未知|\n\n\n我们规定用三个反引号括住代码块\n```\nconst unsigned long int a ;\na = inf ;\n```\n[点击查看源码图](http://ww1.sinaimg.cn/large/006YPp6gly1fvu792e3vuj30wx0npmz5.jpg)\n[点击查看效果图](http://ww1.sinaimg.cn/large/006YPp6gly1fvu6opl5ytj30j80ofagp.jpg)","tags":["工具"]},{"title":"Hexo 更换主题","url":"/2018/09/29/newtheme/","content":"\n简短的关于Hexo主题更新配置的说明\n当你需要更换自己的博客主题时，你需要这样做：\n <!--more-->\n```\n\n\n$ git clone https://*********\n$ npm install hexo-renderer-pug --save\n$ npm install hexo-renderer-sass --save   #下载主题文件和渲染器，如果在下载第三个文件的过程中报错，你可以试试npm国内镜像，或科学上网。\n\ntheme: ****        #打开根目录下的_config.yml,并找到 theme关键字，更新自己主题名称\n$ hexo clean\n$ hexo g           # 生成静态页面\n$ hexo d           # 上传到github\n```\n\n\n","tags":["工具"]}]